

# 機械学習のアルゴリズム
* 最小化すべき目的関数 = 損失関数 + 正則化項
* 損失関数
	* あるデータについて分類に失敗した場合に、その失敗具合に応じて与えるペナルティ
	* これを最小化することで分類を正しく行うのが機械学習
* 正則化項
	* 過学習を緩和させるための仕組み
	* モデルの複雑さを示す指標 ( モデルはあシンプルにしたい )	
* つまり
	* できるだけ確信度を持って間違いを少なくするという項 ( 損失 ) 
	* できるだけシンプルなモデルを採用するという項 ( 正則化項 )
* この2つの和を最小化するということ

# 正則化
* 回帰モデルでは過学習を防ぐために用いられる
	* 闇雲に誤差ばかり最小化するとに過学習に陥ることが多い
* 誤差関数 ( 二乗誤差関数など ) に正則化項を加えて最小化する
* q=1の時をLasso回帰、q=2の時をRidge回帰と呼ぶ
* Lasso回帰に用いられている正則化項をL1ノルム、Ridge回帰をL2ノルムと呼ぶ

## Lasso回帰
* どの説明変数を用いるのが良いか検証するために行う
* Least Absolute Shrinkage and Selection Operator
* L1正則化 ( <-> L2正則化: Ridge )
* サンプル数に比べて、測定項目が非常に大きいが、測定項目のほとんどが予測に寄与していないと考えられる時に非常に有用
  * ある疾患にかかわる遺伝子など
* 必要ない項目の係数を限りなく0にしてしまう
* パラメータの一部を完全に0にする
	* モデルの推定と変数選択を同時に行うことができる
* 特に 次元数 >> データ数 の状況で強力
* 様々な推定アルゴリズムが提案されている
* 微分不可能であり解析的に計算出来ない
	* L2は微分により解析的に計算できる



# 参考
* [old school magic](http://breakbee.hatenablog.jp/entry/2015/03/08/041411)
* [六本木で働くデータサイエンティストのブログ](http://tjo.hatenablog.com/entry/2015/03/03/190000)
* [pandazx's blog](http://pandazx.hatenablog.com/entry/2014/07/11/115123)